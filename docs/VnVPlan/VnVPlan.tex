\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{array}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{enumitem}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
01/11/2024 & 0 & Initial draft\\
08/11/2024 & 0 & Include justification for F.R. test cases, improve  F.R. test case terminology\\
\bottomrule
\end{tabularx}

~\\

\newpage

\tableofcontents

\listoftables

\newpage
\pagenumbering{arabic}

This document outlines the Verification and Validation (VnV) plan for the development 
of an audio-to-sheet music generator. This includes plans to verify the SRS, system 
design, the VnV itself, implementation, and software, as well as a plan for an automated 
testing suite. This document will also describe the specific tests that will be used to 
ensure all Functional and Non-functional Requirements are met.

\section{Symbols, Abbreviations, and Acronyms}
See the \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS} document for additional definitions.

\renewcommand{\arraystretch}{1.2}
\begin{table}[h!]
  \centering
  \caption{Document Definitions}
  \vspace{5pt}
  \begin{tabular}{l l}
    \toprule		
    \textbf{Symbol} & \textbf{Description}\\
    \midrule 
    MG & Module Guide Specification\\
    MIS & Module Interface Specification\\ 
    SRS & Software Requirement Specification\\
    T & Test\\
    VnV & Verification and Validation\\
    \bottomrule
  \end{tabular}
\end{table}

\section{General Information}

\subsection{Summary}

This Verification and Validation Plan describes the testing process for ScoreGen, an 
audio-to-sheet music generator, such that the following core functionalities perform as desired:
\begin{itemize}
  \item Signal processing of audio input
  \item Identification of pitch, rhythm, and timing
  \item Generation of readable and accurate sheet music
\end{itemize}

\subsection{Objectives}

\subsubsection{Desired System Qualities}

The primary objective of this document is to ensure that the audio-to-sheet music system produces 
highly accurate, reliable, and usable output, successfully meeting the needs of all stakeholders. 
Key qualities this VnV will aim to accomplish include the following:
\begin{itemize}
  \item Demonstrate accuracy in music notation
  \item Ease of use amongst users with varying characteristics
  \item Readable, easy to understand outputs
  \item An enjoyable user experience
  \item Efficiency and responsiveness
  \item Portability of the software
\end{itemize}

\subsubsection{Out of Scope Objectives}
In order to meet time and cost requirements, the following objectives will be out of the scope of 
this VnV plan:
\begin{itemize}
  \item Advanced polyphonic and multi-instrumental audio:
  
  Monophonic audio is easily processed using signal analysis, but introducing a multitude of 
  simultaneous signals will require advanced algorithms to separate each note.
  
  \item Cross-platform support:
  
  Majority of the team members will be writing the system’s software on their personal Windows PC. 
  Testing should thus occur in the same environment it is being developed in in order to reduce 
  performance variability of the final system.
  
  \item Advanced music notation, such as dynamics and accents:
  
  Dynamics and accents can be tricky to differentiate, and attempting to identify them is likely to 
  introduce a level of error to the output that is undesired. As well, a beginner musician may feel 
  overwhelmed being faced with a large variety of notation that they don’t recognize.
  
  \item Support for non-Western music notation:
  
  This software will exclusively produce sheet music according to Western music conventions. Expanding 
  to alternative notation conventions would require significant adapting of the output format, and is 
  infeasible given the current scope of the project. 
  
\end{itemize}

\subsection{Challenge Level and Extras}

\subsubsection{Challenge Level}

The ScoreGen project is categorized in the general challenge level, meaning it is not particularly novel 
and requires a senior highschool level or junior undergraduate level of domain knowledge/implementation. 
Projects in this category are required to include ‘extras’ in order to meet the difficulty level expected 
of a final-year capstone course.

\subsubsection{Project Extras}
\begin{itemize}
  \item User manual
  \item Usability testing
  \item GenderMag personas
\end{itemize}

\subsection{Relevant Documentation}

This document is just one of many written to provide a comprehensive overview of the project and its desired 
outcomes. The other project documents include:
\begin{itemize}
  \item \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/DevelopmentPlan/DevelopmentPlan.pdf}{Development Plan} \citep*{ScoreGenDP}: 
  This document acts as a blueprint for the software’s creation as well as the overall structuring of the project itself. Leveraging the information 
  outlined in this document will ensure all verification and validation activities are relevant and within the scope of the project.
  \item \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/ProblemStatementAndGoals/ProblemStatement.pdf}{Problem Statement and Goals} \citep*{ScoreGenPS}:
  This document contains an explicit definition of the problem the ScoreGen system aims to solve. It provides the VnV team with a clear understanding of how 
  tests may shape the project to align with the system’s intended impact.
  \item \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{Software Requirement Specification (SRS)} \citep*{ScoreGenSRS}:
  States the functional and non-functional requirements that this VnV plan is developing tests for.
  \item \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/HazardAnalysis/HazardAnalysis.pdf}{Hazard Analysis} \citep*{ScoreGenHA}:
  Similar to the SRS, this document will guide VnV test plans to ensure all possible hazards to the system are mitigated.
  \item \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/VnVReport/VnVReport.pdf}{Verification and Validation Report} \citep*{ScoreGenVnVReport}:
  This document will be guided entirely by the frameworks described in this VnV plan. Its outcome will depend on the relevance and quality of the described plans.
  \item \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/UserGuide/UserGuide.pdf}{User Guide} \citep*{ScoreGenUG}:
  This guide is intended to be used directly by the end-user to inform them on usage of the software. The VnV aims to 
  verify all use cases and user interactions, and can thus be used to identify the necessary features or steps whose inclusion 
  in the User Guide is critical.
  \item \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{Module Interface Specification (MIS)} \citep*{ScoreGenMIS}:
  This document defines how different software modules within the system interact with one another, making note of all inputs and outputs. These inputs 
  and outputs will require verification outlined in the VnV Plan to ensure feasibility and accurate constraints are put into place.
  \item \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/Design/SoftArchitecture/MG.pdf}{Module Guide (MG)} \citep*{ScoreGenMG}:
  Where the MIS defines module interactions, this document defines the modules themselves. The VnV tests should target all modules in 
  the system - since the MG will be written after the VnV Plan, it may be necessary to revisit this plan and add/remove tests where necessary 
  as the modules are formalized.
\end{itemize}


\section{Plan}

This section will provide formal validation and verification plans for various aspects of the project. 
Starting with a definition of all individuals involved in the VnV process, it goes on to inform the 
reader of VnV plans for the SRS, design, VnV plan itself, implementation, automated tests, and the 
system software.

\subsection{Verification and Validation Team}

The first 4 entries in the below table are the software developers creating the system; they will all 
contribute to VnV activities to ensure the work is evenly distributed, but each has a specific responsibility 
they will oversee. The last 3 entries in the table are not involved in ideation/implementation of VnV, but 
their feedback will shape the final plan.

\begin{table}[h!]
  \centering
  \caption{VnV Team}
  \label{table:vnv}
  \vspace{5pt}
  \begin{tabular}{|p{0.3\textwidth}|p{0.6\textwidth}|}
  \hline
    \textbf{Name} & \textbf{Role} \\
  \hline
    Emily Perica & \textbf{Validation analyst}. Generates test reports and ensures the test results can be 
    linked back to the \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}.\\
  \hline
    Ian Algenio & \textbf{FR tester}. Leads the testing process for functional requirements. \\
  \hline
    Jackson Lippert & \textbf{NFR tester}. Leads the testing process for non-functional requirements. \\
  \hline
    Mark Kogan & \textbf{Verification specialist}. Defines the process for establishing ground truths and 
    ensures all tests being implemented follow the VnV plans. \\
  \hline
    Dr. Martin von \newline Mohrenschildt & \textbf{Project supervisor}. Provides insight and suggests 
    strategies for creating relevant tests. \\
  \hline
    Dr. Spencer Smith & \textbf{Course supervisor}. Similar to the project supervisor; provides insight and 
    guides the team in creating a robust VnV plan.\\
  \hline
    Hunter Ceranic & \textbf{Teaching assistant}. Will provide feedback to the team on the feasibility, scope, 
    and overall quality of the VnV plan.\\
  \hline
  \end{tabular}
\end{table}

\subsection{SRS Verification Plan}
\label{sec:srs_verification}

To verify the SRS for our sheet music generation project, we plan to use a combination of informal feedback and 
structured reviews to ensure accuracy and alignment with the project's goals. The following subsections describe 
these approaches - the combination of peer and supervisor reviews, structured meetings, and issue tracking will 
provide comprehensive coverage for SRS verification.

\subsubsection{Ad Hoc Feedback from Peers}
We will request informal reviews from our primary reviewers (team 8). Their feedback will help identify potential 
ambiguities or areas needing clarification. We will ask them to focus on the clarity and feasibility of requirements, 
ensuring alignment with the intended functionality of the app.

\subsubsection{Supervisor Review Meeting}

\textbf{Structured Review Meeting}: We plan to conduct a concise, structured meeting with our project supervisor, Dr. 
von Mohrenschildt, to leverage his expertise effectively while respecting his time constraints. This meeting will 
include the following steps:

\begin{enumerate}
    \item \textbf{Overview Presentation}: We will present a high-level overview of our initial requirements, focusing 
    on the key features and functionalities we believe are essential for the project. The purpose of this presentation 
    is to quickly bring the supervisor up to speed on the project's direction.
    
    \item \textbf{Discussion on Feasibility and Priority}: We will ask the supervisor to evaluate the feasibility of 
    the proposed requirements, help identify any that are overly ambitious or unrealistic, and highlight those that 
    are core to the project’s success. This will help us prioritize our work based on expert advice.
    
    \item \textbf{Focused Questions}: To guide the discussion and make the best use of the supervisor’s time, we will 
    prepare targeted questions that focus on critical project areas. Examples include:
    \begin{itemize}
        \item Which requirements are technically challenging or might present significant roadblocks?
        \item Are there any high-priority requirements that we might have overlooked?
        \item Which elements align closely with the project’s objectives, and which might be deferred?
    \end{itemize}
    
    \item \textbf{Utilizing Issue Tracker}: To maintain transparency and track action items, we will record the supervisor’s 
    feedback and any new insights or tasks identified during the meeting in our issue tracker. This will enable us to follow 
    up efficiently and ensure that the supervisor's input is incorporated into subsequent iterations of our requirements document.
\end{enumerate}


\subsubsection{Issue Tracking System}
We will use the \href{https://github.com/users/emilyperica/projects/1}{git issue tracker} to log any concerns, suggestions, or 
changes requested by reviewers. This system will help maintain a clear record of all feedback and revisions made to the SRS, 
ensuring transparency and accountability in addressing reviewer comments.


\subsection{Design Verification Plan}

To ensure the design of the sheet music generation app meets all functional and non-functional requirements, the following 
verification plan will be implemented:

\subsubsection*{Peer Review and Feedback}
The design will undergo reviews by our classmates, focusing on key design elements such as the user interface and algorithm 
selection for audio-to-sheet music conversion. Feedback from peers will help identify usability issues, potential improvements, 
and ensure that the design aligns with the project goals.

\subsubsection*{Checklist-Based Review}
A checklist will be created to guide reviewers in assessing each design aspect:
\begin{itemize}
    \item Are the core algorithms well-suited for the required audio-to-sheet music conversion?
    \item Are system constraints (e.g., processing time, memory usage) addressed in the design?
    \item Is the design modular and maintainable?
\end{itemize}
This checklist will help standardize feedback, ensuring a thorough and consistent review process.

\subsubsection*{Supervisor Review}
In addition to peer review, we will present the design to our project supervisor for further validation. The supervisor will be 
provided with the design documents and checklist before the review session to facilitate a structured examination of each design 
component. Key discussion points will include scalability, performance optimization, and potential risks. 

\subsubsection*{Issue Tracking and Follow-Up}
Git Issues will be used to log any design issues or recommendations identified during the review process. This will provide a 
centralized record of all feedback and allow for systematic tracking of design revisions, ensuring that each suggestion is 
addressed or appropriately documented.

\vspace{10pt}
\noindent This design verification plan, incorporating both peer and supervisor feedback along with checklist-guided reviews and 
structured follow-up, will ensure the design is robust, user-friendly, and aligns with project requirements.


\subsection{Verification and Validation Plan Verification Plan}

\textbf{Verification and Validation Plan Checklist}:

\begin{itemize}
    \item \textbf{Plan Review by Peers}: Ensure the verification and validation plan is reviewed by classmates to gain diverse insights 
    and identify room for improvement.
    \item \textbf{Requirement Coverage}: Verify that all requirements are adequately covered by the verification and validation plan, 
    ensuring no critical areas are omitted.
    \item \textbf{Traceability}: Confirm traceability of each verification and validation activity to specific project requirements to 
    maintain alignment with project goals.
    \item \textbf{Clarity and Completeness}: Review the plan for clarity, making sure all instructions and testing procedures are clear 
    and complete for future reference.
\end{itemize}


\subsection{Implementation Verification Plan}

The Implementation Verification Plan outlines the strategies and methodologies we will employ to verify the correctness and quality of 
our implementation.

\begin{itemize}
    \item \textbf{Unit Testing Plan}: A comprehensive unit testing plan (\hyperref[sec:unit_tests]{section 5}) will be developed, 
    which includes a detailed list of tests designed to verify the functionality of individual components of the system. Each unit 
    test will target specific functions or classes to ensure they perform as expected under various conditions. The results of 
    these tests will be crucial for identifying and resolving issues early in the development process.

    \item \textbf{Static Verification Methods}: In addition to dynamic testing, we will implement several static verification techniques 
    to enhance the quality of our code. These methods allow us to analyze the code without executing it, thereby identifying potential 
    issues at an early stage. The techniques we plan to use include:
    \begin{itemize}
        \item \textbf{Code Walkthroughs}: We will conduct peer reviews in the form of code walkthroughs, where team members will 
        systematically review code segments. This collaborative approach encourages knowledge sharing and helps catch defects or design 
        flaws that may not be evident to the original developer.
        \item \textbf{Static Analyzers}: We will employ static analysis tools to automatically analyze the code for common programming 
        errors, potential security vulnerabilities, and adherence to coding best practices. These tools provide insights that can help 
        us improve code quality and reduce technical debt.
    \end{itemize}
\end{itemize}

\noindent By integrating both unit testing and static verification methods into our implementation verification plan, we aim to ensure 
that our code is robust, maintainable, and free from critical defects. The combination of unit testing and static verification not only 
enhances the reliability of our implementation but also streamlines the development process by identifying issues early on. Implementing
this plan will reduce the possibility of accruing technical debt, and overall increase the ease of software development.

\subsection{Automated Testing and Verification Tools}

For our project, we plan to utilize GitHub Actions to create a continuous integration pipeline, which will automate the build and 
testing processes. We will employ the Google Test framework for unit testing, as it provides a robust and easy-to-use interface 
for writing and executing C++ test cases.

In addition to unit testing, we will implement linters to verify coding standards and maintain consistency across the codebase. 
The coding standards we will follow include:

\begin{itemize}
    \item \href{https://google.github.io/styleguide/cppguide.html}{Google's C++ Style Guide} \citep*{GoogleCppStyleGuide} for C++.
    \item \href{https://peps.python.org/pep-0008/}{PEP8 Style Guide} \citep*{PEP8PythonStyleGuide}: for Python.
    \item \href{https://getbem.com/}{BEM Methodology} \citep*{BEMMethodology}: for HTML and CSS.
\end{itemize}

We will summarize code coverage metrics by integrating a coverage tool like gcov with our CI pipeline, allowing us to visualize 
the effectiveness of our tests and identify untested code paths.

\subsection{Software Validation Plan}

For our project, we plan to validate the functionality of our application using imported music files. These music files will serve 
as test cases to ensure that our software correctly processes and generates sheet music from audio inputs.

To further validate the product we will conduct user testing to gather feedback on the functionality and usability of our application, 
allowing us to make improvements based on user experiences.

There will also be a quality assurance testing effort to ensure a robust adherence to the defined SRS requirements, as further defined 
in the \hyperref[sec:srs_verification]{SRS Verification Plan}

\section{System Tests}

System tests are categorized into two subsections. Section 4.1 lists test cases for functional requirements defined in 
the SRS, and section 4.2 lists test cases for nonfunctional requirements defined in the SRS. 

\subsection{Tests for Functional Requirements}
Tests for the functional requirements of the application naturally follow the division of the major types of functional requirements
in section 9 of the \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}. Thus, tests for each major 
type are grouped similarly, resulting in five subcategories.

  \subsubsection{Input Handling}
  \begin{enumerate}
    \item \textbf{Test for Correct Audio File Formats} \\
      \newline
      \textbf{Test ID:} FR-AR1-3-1 \\
      \textbf{Control:} Automatic \\
      \textbf{Initial State:} Application is running and idle, awaiting audio file upload from the user. \\
      \textbf{Input:} Audio file (e.g., \texttt{.WAV}, \texttt{.MP3}) \\
      \textbf{Output:} File acceptance without errors, entrance into file processing state. \\
      \textbf{Test Case Derivation:} For validation of functional requirements FR-AR1 and FR-AR3 in section 9.1 of the 
      \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}. \\
      \textbf{Justification:} This test case ensures the application only accepts supported file formats and only begins processing 
      these formats. This test case acts as a preventative measure against further error progogation through the file processing stage.\\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Select a prepared audio sample file.
          \item Upload the audio file for transcription.
          \item Confirm the application accepts the file through success message or by entrance into the file processing state.
      \end{enumerate}
  
  \item \textbf{Test for Incorrect File Formats} \\
    \newline
    \textbf{Test ID:} FR-AR1-3-2 \\
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Application is open and idle, awaiting audio file upload from the user. \\
    \textbf{Input:} Non-audio file (e.g., \texttt{.PDF}, \texttt{.MP4}, \texttt{.JPEG}, etc.) \\
    \textbf{Output:} Denial of upload attempt with error message. \\
    \textbf{Test Case Derivation:} For validation of functional requirements FR-AR1 and FR-AR3 in section 9.1 of the
    \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}. \\
    \textbf{Justification:} Complements test case FR-AR1-3-1. It will ensure that various, unsupported file formats are 
    not accepted by the application. \\
    \textbf{How Test Will Be Performed:}
    \begin{enumerate}
        \item Select a prepared file of an unsupported format (i.e. non-audio file format).
        \item Upload the file for transcription.
        \item Confirm the application denies the file and provides an error message that specifies the unsupported file format.
    \end{enumerate}
  
  \item \textbf{Test User Device Microphone} \\
  \newline
  \textbf{Test ID:} FR-AR2 \\
  \textbf{Control:} Manual \\
  \textbf{Initial State:} Application is open and idle, user has navigated to the audio recording interface and microphone 
  permissions have been granted. \\
  \textbf{Input:} Audio recorded by the user device’s microphone. \\
  \textbf{Output:} The application captures the correct audio and saves it in a format processable by the application 
  (e.g., \texttt{.WAV}). \\
  \textbf{Test Case Derivation:} For validation of functional requirement FR-AR2 in section 9.1 of the 
  \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}. Guarentees that in addition to file uploads, 
  the user is able to capture raw, custom audio with their device hardware. \\
  \textbf{Justification:} It also validates that the audio is saved in a compatitble format for processing.\\
  \textbf{How Test Will Be Performed:}
  \begin{enumerate}
      \item Navigate to the application’s audio recording interface.
      \item Start a recording using the application’s “Record” element.
      \item Wait for 10 seconds and stop the recording.
      \item Confirm that the audio was recorded through:
      \begin{itemize}
          \item Playback using an audio player on the device.
          \item File metadata analysis (duration, file size, etc.).
          \item Waveform inspection using an external tool (e.g., Audacity).
      \end{itemize}
      \item Confirm that the captured audio matches the expected input via playback, metadata, and/or waveform inspection.
  \end{enumerate}
  
  \item \textbf{Test Generated Score Alignment with Selected Instrument} \\
    \newline
    \textbf{Test ID:} FR-AR4 \\
    \textbf{Control:} Automatic \\
    \textbf{Initial State:} Application is running and idle, pre-transcription state awaiting input. \\
    \textbf{Input:} Sample audio file or user-recorded audio and selected instrument type. \\
    \textbf{Output:} A generated sheet music file in MusicXML format that aligns with the selected instrument’s key signature 
    and note pitches. \\
    \textbf{Test Case Derivation:} For validation of functional requirement FR-AR4 in section 9.1 of the 
    \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}.\\
    \textbf{Justification:} Matching an instrument's specific key and pitch requirements is essential for sheet music accuracy. \\
    \textbf{How Test Will Be Performed:}
    \begin{enumerate}
        \item Select and submit an instrument type within the application.
        \item Upload an audio input for the selected instrument.
        \item Upon transcription completion, parse the generated score.
        \item Verify that the key signature and note pitches match the expected result for the selected instrument.
    \end{enumerate}
  \end{enumerate}

  \subsubsection{Signal Processing and Element Identification}
  \begin{enumerate}
    \item \textbf{Test Effect of Increased Noise in Audio Input} \\
      \newline
      \textbf{Test ID:} FR-SP1 \\
      \textbf{Control:} Automatic \\
      \textbf{Initial State:} Application is running and idle, awaiting file upload from the user. \\
      \textbf{Input:} Two sample audio files—one unedited, the other with ~10\% noise interference. \\
      \textbf{Output:} Two identical sheet music files in MusicXML format. \\
      \textbf{Test Case Derivation:} For validation of functional requirement FR-SP1 in section 9.2 of the 
      \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}. \\
      \textbf{Justification:} Confirms that the application can handle expected levels of background noise in input 
      audio, and that it can maintain accuracy despite the noise.\\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Upload the unedited audio file to the application and generate a sheet music file.
          \item Upload the noisy audio file and generate another sheet music file.
          \item Parse both files and identify discrepancies, if any.
      \end{enumerate}
    
    \item \textbf{Test for Pitch and Rhythm Identification} \\
      \newline
      \textbf{Test ID:} FR-SP2 \\
      \textbf{Control:} Manual \\
      \textbf{Initial State:} Application is running and idle, awaiting audio input from the user. \\
      \textbf{Input:} Sample audio file or user-recorded audio with a known sheet music equivalent. \\
      \textbf{Output:} Sheet music correctly identifying all notes in the input audio. \\
      \textbf{Test Case Derivation:} For validation of functional requirement FR-SP2 in section 9.2 of the 
      \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}. \\
      \textbf{Justification:} Verfies the transcription stage as well as the pitch and rhythm identification algorithms employed
      by the applications implementation.\\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Prepare an audio file containing an ascending and descending C major scale.
          \item Upload the audio input to the application.
          \item Generate and save the sheet music in a viewable file format.
          \item Compare the generated sheet music visually with the sample sheet music for note discrepancies.
      \end{enumerate}

    \item \textbf{Test for Key Signature and Time Signature Identification} \\
      \newline
      \textbf{Test ID:} FR-SP3 \\
      \textbf{Control:} Automatic \\
      \textbf{Initial State:} Application is running and idle, awaiting audio input from the user. \\
      \textbf{Input:} Sample or user-recorded audio file that has a known key signature and time signature. \\
      \textbf{Output:} Sheet music that has the same key signature and time signature as the input’s sheet music. \\
      \textbf{Test Case Derivation:} For validation of functional requirement FR-SP3 in section 9.2 of the 
      \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}. \\
      \textbf{Justification:} Further verifies the transcription stage and accuracy of other sheet music elements.\\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Upload the sample file or recorded audio file to the application.
          \item Save the generated sheet music in MusicXML file format.
          \item Parse the generated file and extract the key signature and time signature.
          \item Compare the extracted signatures to the known input’s signatures.
      \end{enumerate}

    \item \textbf{Test for Polyphonic Audio Identification} \\
      \newline
      \textbf{Test ID:} FR-SP5 \\
      \textbf{Control:} Automatic \\
      \textbf{Initial State:} Application is running and idle, awaiting audio input from the user. \\
      \textbf{Input:} Sample file or user-recorded audio that contains known chords. \\
      \textbf{Output:} Sheet music file in MusicXML format that matches the input file’s chords. \\
      \textbf{Test Case Derivation:} For validation of functional requirement FR-SP5 in section 9.2 of the 
      \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}. \\
      \textbf{Justification:} Tests the application's ability to decipher and transcribe multiple simultaneous notes
      in the audio input. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Upload the sample audio file to the application.
          \item Ensure the sample audio file includes at least two distinct chords; single notes may or may not be interleaved.
          \item Save the generated sheet music in MusicXML file format.
          \item Parse the generated file.
          \item Compare the processed and identified chords to the known input audio chords and notes.
      \end{enumerate}
    \item \textbf{Test for Monophonic Audio Identification} \\
    Subsumed by test case FR-SP2 (see section 4.1.2.2)
  \end{enumerate}

  \subsubsection{Sheet Music Generation}
  \begin{enumerate}
  \item \textbf{Test General Notation and Layout} \\
    \newline
    \textbf{Test ID:} FR-SMG1 \\
    \textbf{Control:} Automatic with manual inspection \\
    \textbf{Initial State:} Application is running and idle, awaiting audio input from the user. \\
    \textbf{Input:} Sample or user-recorded audio file with equivalent sheet music available. \\
    \textbf{Output:} Sheet music using the same layout and notation as the input. \\
    \textbf{Test Case Derivation:} For validation of functional requirement FR-SMG1 in section 9.3 of the 
    \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}. \\
    \textbf{Justification:} Ensures readability and usability, using improper notation and layout defeats the purpose of 
    generating sheet music. \\
    \textbf{How Test Will Be Performed:}
    \begin{enumerate}
        \item Upload the sample or user-recorded audio file for transcription.
        \item Save the generated sheet music in MusicXML format and a viewable document format.
        \item Check for discrepancies:
        \begin{itemize}
            \item Parse the MusicXML file and compare it to the input file.
            \item View the document formatted file (e.g., \texttt{.PDF}) and compare it to the input’s sheet music.
        \end{itemize}
    \end{enumerate}
  
  \item \textbf{Test Instrument Specific Concert Pitch} \\
    \newline
    \textbf{Test ID:} FR-SMG2 \\
    \textbf{Control:} Manual \\
    \textbf{Initial State:} Application is running and idle, awaiting audio input from the user. \\
    \textbf{Input:} Two audio files—one from a non-transposing instrument and another from a transposing instrument. \\
    \textbf{Output:} Two sets of sheet music that structurally and visually match their corresponding instrument’s concert pitch. \\
    \textbf{Test Case Derivation:} For validation of functional requirement FR-SMG2 in section 9.3 of the 
    \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}. \\
    \textbf{Justification:} Some instruments are transposing while others are non-transposing, this test case confirms the application's 
    ability to support a broad range of instruments.\\
    \textbf{How Test Will Be Performed:}
    \begin{enumerate}
        \item Upload a non-transposing instrument’s audio file for transcription and save the generated sheet music in a 
        viewable file format.
        \item Upload a transposing instrument’s audio file for transcription and save the generated sheet music in a viewable 
        file format.
        \item Compare the two sets of sheet music to ensure appropriate notes are in concert pitch relative to the input sheet music.
    \end{enumerate}
  
  \item \textbf{Test Post-Processing Edit and View Functionalities} \\
    \newline
    \textbf{Test ID:} FR-SMG3 \\
    \textbf{Control:} Manual \\
    \textbf{Initial State:} Application has just finished processing and transcribing audio input. \\
    \textbf{Input:} Edit requests, save requests, open requests. \\
    \textbf{Output:} Viewable file containing sheet music that reflects the requested edits. \\
    \textbf{Test Case Derivation:} For validation of functional requirement FR-SMG3 in section 9.3 of the 
    \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}. \\
    \textbf{Justification:} Ensures that users can change any elements of the sheet music and that these 
    changes are reflected properly by the application.\\ 
    \textbf{How Test Will Be Performed:}
    \begin{enumerate}
        \item View the generated sheet music in the application.
        \item Perform three edit operations:
        \begin{itemize}
            \item Note deletion.
            \item Note addition.
            \item Note pitch and/or duration change.
        \end{itemize}
        \item Save the edited sheet music in a viewable file format.
        \item Open and view the edited sheet music file to confirm that changes are present and correct.
    \end{enumerate}
  \end{enumerate}
  \subsubsection{User Interface (UI)}
  \begin{enumerate}
    \item \textbf{Test Application Feedback After Audio File is Uploaded} \\
      \newline
      \textbf{Test ID:} FR-UI1 \\
      \textbf{Control:} Automatic \\
      \textbf{Initial State:} Application is running and idle, awaiting audio input from the user. \\
      \textbf{Input:} A sample audio file. \\
      \textbf{Output:} Visual cue(s) on the GUI in 2 seconds or less. \\
      \textbf{Test Case Derivation:} For validation of functional requirement FR-UI1 in section 9.4 of the 
      \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}. \\
      \textbf{Justification:} Validates the funcationality that contributes to the non-functional requirement of
      human-centered design principles (see Section 4.2.2.1). One of the four fundamental principles is feedback, without 
      the proper creation and display of visual feedback cues, the application does not fulfill this principle to the 
      best of its ability.\\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Upload a sample audio file to the application.
          \item Start a timer and detect visual element changes on the GUI using a testing framework (e.g., Jest).
          \item Confirm the following conditions are met:
          \begin{itemize}
              \item The appropriate visual cue is detected.
              \item The elapsed time from upload start to display of visual feedback is at most 2 seconds.
          \end{itemize}
      \end{enumerate}
    
    \item \textbf{Test Availability of System Documentation} \\
      \newline
      \textbf{Test ID:} FR-UI2 \\
      \textbf{Control:} Manual \\
      \textbf{Initial State:} Application is running and idle. \\
      \textbf{Input:} Text-search queries. \\
      \textbf{Output:} Navigation to appropriate documentation/user guide sections. \\
      \textbf{Test Case Derivation:} For use during user testing to meet fit criteria for functional requirement FR-UI2 in section 
      9.4 of the \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}. \\
      \textbf{Justification:} Further supports usability and humanity testing (see Section 4.2.2). Without documentation, 
      users may find navigating and using the appliation difficult.\\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Navigate to the location of user documentation in the application.
          \item Perform a broad text search for major application features or for anything the user requires clarification on.
      \end{enumerate}
    
    \item \textbf{Test User Feedback Report Mechanism} \\
      \newline
      \textbf{Test ID:} FR-UI3 \\
      \textbf{Control:} Automatic \\
      \textbf{Initial State:} Application is running and idle, prepared to receive input through the feedback mechanism. \\
      \textbf{Input:} Plain text. \\
      \textbf{Output:} GUI submission success cue and message, return of the input text. \\
      \textbf{Test Case Derivation:} For validation of functional requirement FR-UI3 in section 9.4 of the 
      \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}. \\
      \textbf{Justification:} Ensures users are able to submit basic feedback or issues and that the application provides adequate
      feedback in the form of confirmation of a successful submission.\\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Submit a plain text report via the user feedback mechanism to the development team (e.g., through email).
          \item Parse through inbox messages for user feedback reports and confirm the reception of the submitted report and 
          entire plain text input.
      \end{enumerate}
  \end{enumerate}
  \subsubsection{Save/Load}
  \begin{enumerate}
    \item \textbf{Test Application Save Function} \\
      \newline
      \textbf{Test ID:} FR-SL1 \\
      \textbf{Control:} Manual \\
      \textbf{Initial State:} Post-audio processing state with generated sheet music file(s) and original audio files accessible 
      for download. \\
      \textbf{Input:} Request to save file(s) to local drive. \\
      \textbf{Output:} Non-corrupt file(s) in destination directories. \\
      \textbf{Test Case Derivation:} For validation of functional requirement FR-SL1 in section 9.5 of the 
      \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}. \\
      \textbf{Justification:} Confirms that users are able to track their progress and save sheet music and audio files to their local 
      storage without data corruption.\\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Transcribe a sample audio file using the application.
          \item Request to download both the original audio and the generated sheet music to a directory on the local drive.
          \item Compare the contents of the downloaded files to their original copies to check for consistency.
      \end{enumerate}
    
    \item \textbf{Test Application’s Ability to Load Existing Files} \\
      \newline
      \textbf{Test ID:} FR-SL2 \\
      \textbf{Control:} Manual \\
      \textbf{Initial State:} Application running and idle, waiting for user input to modify and/or view. \\
      \textbf{Input:} An existing audio file or existing file containing previously generated sheet music. \\
      \textbf{Output:} Successful loading of files without errors. \\
      \textbf{Test Case Derivation:} For validation of functional requirement FR-SL2 in section 9.5 of the 
      \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS}. \\
      \textbf{Justification:} Complements test case FR-SL1. Ensures users can load previously saved sheet music or 
      audio files into the application without errors.\\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Request to load an existing file on the local drive.
          \item Manually inspect the opened file in the editor to ensure the files appear as they were saved.
      \end{enumerate}
  \end{enumerate}
\subsection{Tests for Nonfunctional Requirements}

The following tests are for nonfunctional requirements. There are a few things to note about the tests:
\begin{enumerate}
  \item Tests will be carried out by a member of our team (led by NFR tester Jackson Lippert as shown in table \ref{table:vnv}), 
  the tests have not been assigned at this time as we are unsure what the future of testing will look like at this time.
  \item Wherever the tests refer to a 'sample group of people' (such as for usability testing), we will decide at a later time 
  who that will be.
  \item Usability Tests are left semi-ambiguous at this time to give us room to expand in the future when implementation details 
  are better known.
\end{enumerate}

\subsubsection{Look and Feel Testing}
\begin{enumerate}
    \item \textbf{Test for LF-A1 Discoverability} \\
      \newline
      \textbf{Test ID:} LF-A1 \\
      \textbf{Type:} Usability, Dynamic, Manual \\
      \textbf{Initial State:} The application is launched, showing the main interface. \\
      \textbf{Input/Condition:} The user views the interface for the first time without guidance. \\
      \textbf{Output/Result:} 75\% of users should be able to locate the interactive element that initiates the score generation process. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Recruit a sample group of users unfamiliar with the app and explain that they need to initiate the score generation 
          process.
          \item Record if each user is able to locate the score generation button within 10 seconds.
          \item Ask users to describe why they selected the specific element as the score generation option.
      \end{enumerate}
      \textbf{Success Criterion:} If 75\% or more of the users identify the correct element, the test is marked as passed.

    \item \textbf{Test for LF-A2 Colour Cohesion} \\
      \newline
      \textbf{Test ID:} LF-A2 \\
      \textbf{Type:} Static, Manual, Visual Inspection \\
      \textbf{Initial State:} The application interface is fully designed. \\
      \textbf{Input/Condition:} Inspect the interface to verify colour usage. \\
      \textbf{Output/Result:} To meet the fit criterion, the colour scheme should have at least three distinct colours: primary, 
      secondary, and accent. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Identify and document the primary, secondary, and accent colours as specified in the app's design guidelines.
          \item Conduct a visual inspection of each screen and interactive element within the app to confirm adherence to these colours.
          \item Verify there are no unintended or extraneous colours used across the app.
      \end{enumerate}
      \textbf{Success Criterion:} If all screens consistently use the defined colour palette, the test passes.

    \item \textbf{Test for LF-A3 Resolution} \\
      \newline
      \textbf{Test ID:} LF-A3 \\
      \textbf{Type:} Functional, Manual, Dynamic \\
      \textbf{Initial State:} All graphics are loaded within the application. \\
      \textbf{Input/Condition:} Inspect images to verify resolution. \\
      \textbf{Output/Result:} All graphics should display at 720p resolution or higher. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Compile a list of all graphical assets used in the app, including their file locations.
          \item Use image inspection tools to verify the resolution of each image.
          \item Document the resolution of each graphic and flag any below 720p for replacement.
      \end{enumerate}
      \textbf{Success Criterion:} If all images meet or exceed 720p resolution, the test passes.

    \item \textbf{Test for LF-S1 Authority and Trust} \\
      \newline
      \textbf{Test ID:} LF-S1 \\
      \textbf{Type:} Usability, Dynamic, Survey-Based \\
      \textbf{Initial State:} The user has interacted with the application once. \\
      \textbf{Input/Condition:} The user provides feedback on their perception of the app’s authority and trustworthiness. \\
      \textbf{Output/Result:} 70\% of surveyed users report feeling that the application is reliable and trustworthy. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item After a user’s initial interaction with the app, provide them with a survey containing the following questions:
          \begin{itemize}
              \item Q1: On a scale of 1-5, how trustworthy does this app feel when handling your data? (1 = Not Trustworthy, 5 = 
              Very Trustworthy)
              \item Q2: Do you feel confident using this app for your music notation needs? (Yes/No)
              \item Q3: Please briefly explain any factors that contributed to your level of trust in the app.
          \end{itemize}
          \item Collect responses and analyze the data to calculate the percentage of users rating the app as 4 or 5 for 
          trustworthiness in Q1 and answering "Yes" to Q2.
      \end{enumerate}
      \textbf{Success Criterion:} If 70\% or more users rate the app 4 or higher on trustworthiness and respond “Yes” to Q2, the 
      test passes.
\end{enumerate}

\subsubsection{Usability and Humanity Testing}
\begin{enumerate}
    \item \textbf{Test for UH-EOU1 Human-Centered Design (HCD)} \\
      \newline
      \textbf{Test ID:} UH-EOU1 \\
      \textbf{Type:} Usability, Static, Inspection-Based \\
      \textbf{Initial State:} The app interface is fully developed, adhering to HCD principles \citep*{HCD}. \\
      \textbf{Input/Condition:} Inspect the app interface for compliance with the four fundamental HCD principles. \\
      \textbf{Output/Result:} The user interface must visibly incorporate all four HCD principles. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Review the app’s design documentation to verify its adherence to the HCD principles: visibility, consistency, 
          user control, and feedback.
          \item Conduct an inspection-based evaluation of the interface with usability experts, identifying examples of each HCD 
          principle in action.
      \end{enumerate}
      \textbf{Success Criterion:} The app passes if all four HCD principles are clearly implemented and documented within the interface.

    \item \textbf{Test for UH-PI1 Language} \\
      \newline
      \textbf{Test ID:} UH-PI1 \\
      \textbf{Type:} Functional, Static, Manual \\
      \textbf{Initial State:} The app is fully localized with text in Canadian English (en-CA). \\
      \textbf{Input/Condition:} Inspect all text, labels, and instructions in the app. \\
      \textbf{Output/Result:} All text should be presented in Canadian English, with no grammatical or spelling errors. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Conduct a manual review of all textual elements within the app, ensuring they are correctly localized in Canadian 
          English.
          \item Use a grammar and spell-check tool to verify accuracy.
      \end{enumerate}
      \textbf{Success Criterion:} The test is passed if all text is in Canadian English and no errors are found.

    \item \textbf{Test for UH-L1 Music Theory Familiarity} \\
      \newline
      \textbf{Test ID:} UH-L1 \\
      \textbf{Type:} Usability, Dynamic, User Testing \\
      \textbf{Initial State:} The user is unfamiliar with the app and has no formal music theory background. \\
      \textbf{Input/Condition:} The user has 10 minutes to explore and use the app without guidance. \\
      \textbf{Output/Result:} 95\% of users without a music theory background are able to generate a score sheet within the allotted 
      time. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Recruit a sample of users with no formal music theory background.
          \item Allow users 10 minutes to familiarize themselves with the interface.
          \item Observe and document whether each user is able to generate a score sheet within this period.
      \end{enumerate}
      \textbf{Success Criterion:} The test is passed if 95\% or more of the users complete score generation without assistance.

    \item \textbf{Test for UH-UP1 Icon Identification} \\
      \newline
      \textbf{Test ID:} UH-UP1 \\
      \textbf{Type:} Usability, Dynamic, Survey-Based \\
      \textbf{Initial State:} The app displays interactive elements with unlabeled icons. \\
      \textbf{Input/Condition:} Users attempt to identify the function of each interactive icon. \\
      \textbf{Output/Result:} At least 70\% of icons are accurately identified by users. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Present a sample of users with the app interface and ask them to identify the function of each interactive icon 
          without clicking or interacting with them.
          \item Record user responses and compare them to the correct icon functions.
      \end{enumerate}
      \textbf{Success Criterion:} If at least 70\% of the icons are correctly identified by the majority of users, the test is passed.

    \item \textbf{Test for UH-UP2 Information Hiding} \\
      \newline
      \textbf{Test ID:} UH-UP2 \\
      \textbf{Type:} Functional, Static \\
      \textbf{Initial State:} The app is fully developed and ready for inspection. \\
      \textbf{Input/Condition:} Inspect the app interface and accessible areas. \\
      \textbf{Output/Result:} No user-accessible elements reveal implementation-specific or algorithmic details. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Conduct a static inspection of all user-accessible components in the interface, including settings, menus, and tooltips.
          \item Confirm that no elements or descriptions expose the internal implementation or processing details.
      \end{enumerate}
      \textbf{Success Criterion:} The test is passed if no user-accessible components reveal underlying algorithms or implementation 
      details.

    \item \textbf{Test for UH-A1 Web Content Accessibility Guidelines (WCAG) Compliance} \\
      \newline
      \textbf{Test ID:} UH-A1 \\
      \textbf{Type:} Accessibility, Static and Dynamic, Automated and Manual \\
      \textbf{Initial State:} The app interface is complete and ready for accessibility testing. \\
      \textbf{Input/Condition:} Perform an accessibility audit using automated tools and manual checks. \\
      \textbf{Output/Result:} The app meets or exceeds WCAG 2.1 Level AA compliance \citep*{WCAG21}. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Run automated accessibility testing tools to identify any WCAG Level AA compliance issues, including alt text for 
          images, colour contrast, keyboard navigability, etc.
          \item Conduct manual testing for areas not covered by automated tools, such as text readability and interaction.
          \item Document any issues and confirm adherence to all WCAG 2.1 Level AA standards.
      \end{enumerate}
      \textbf{Success Criterion:} The test is passed if the app fully complies with WCAG 2.1 Level AA standards, confirmed by both 
      automated and manual testing.
\end{enumerate}

\subsubsection{Performance Testing}
\begin{enumerate}
    \item \textbf{Test for PR-SL1 User Interface Response Time} \\
      \newline
      \textbf{Test ID:} PR-SL1 \\
      \textbf{Type:} Performance, Dynamic \\
      \textbf{Initial State:} The application is open and ready for user interaction. \\
      \textbf{Input/Condition:} Perform multiple interactions, such as menu navigation and input processing. \\
      \textbf{Output/Result:} The app should respond within 2 seconds for 90\% of interactions, with no interaction taking longer 
      than 5 seconds. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Conduct a series of interactions across the application, including navigation through menus and processing various 
          inputs.
          \item Record response times for each interaction.
          \item Calculate the percentage of interactions that respond within 2 seconds and confirm that no response exceeds 5 seconds.
      \end{enumerate}

    \item \textbf{Test for PR-SL2 File Import and Export Speed} \\
      \newline
      \textbf{Test ID:} PR-SL2 \\
      \textbf{Type:} Performance, Dynamic \\
      \textbf{Initial State:} The app is ready to import and export files. \\
      \textbf{Input/Condition:} Test with music files up to 100MB in size for both import and export functions. \\
      \textbf{Output/Result:} The app should complete imports and exports within a reasonable time frame for at least 95\% of operations. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Import and export a range of music files up to 100MB in size, timing each operation. An example music file is given 
          \href{https://github.com/emilyperica/ScoreGen/blob/main/test/TestingDatasets/piano-samples/sample-songs/hot-cross-buns-piano-solo.wav}{here}.
          \item Document the time taken for each operation and calculate the percentage of imports and exports that complete within an acceptable time.
          \item Confirm that 95\% of operations meet the expected time frame.
      \end{enumerate}

    \item \textbf{Test for PR-SC1 Epilepsy Safety} \\
      \newline
      \textbf{Test ID:} PR-SC1 \\
      \textbf{Type:} Accessibility, Static, Automated \\
      \textbf{Initial State:} The application is open with all visual elements loaded. \\
      \textbf{Input/Condition:} Inspect graphical interface elements for compliance with WCAG 2.1 guidelines on flashing content 
      \citep*{WCAG21}. \\
      \textbf{Output/Result:} No visual elements should flash at a rate of more than 3 flashes per second. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Use accessibility tools to scan the interface for flashing content.
          \item Verify that all visual effects adhere to the flash rate limitation.
          \item Document any elements that exceed the flash rate and adjust them to ensure compliance.
      \end{enumerate}

    \item \textbf{Test for PR-SC2 Instrument Input Setup} \\
      \newline
      \textbf{Test ID:} PR-SC2 \\
      \textbf{Type:} Functional, Usability \\
      \textbf{Initial State:} The app is open, with the instrument input configuration tutorial available. \\
      \textbf{Input/Condition:} Use the tutorial to set up an external instrument or microphone. \\
      \textbf{Output/Result:} The setup should be completed successfully on the first attempt with a 95\% success rate across 
      user testing. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Guide a sample group of users through the step-by-step input configuration tutorial.
          \item Track the completion success rate for each user on their first attempt.
          \item Confirm that at least 95\% of users complete the setup successfully on their first attempt.
      \end{enumerate}

    \item \textbf{Test for PR-PA1 Pitch Detection Accuracy} \\
      \newline
      \textbf{Test ID:} PR-PA1 \\
      \textbf{Type:} Functional, Performance \\
      \textbf{Initial State:} The app is configured for audio input from diverse instruments. \\
      \textbf{Input/Condition:} Test pitch detection with a range of instruments and note ranges. Test samples found 
      \href{https://github.com/emilyperica/ScoreGen/tree/main/test/TestingDatasets}{here.}\\
      \textbf{Output/Result:} The pitch detection accuracy should be within a 1\% error margin across all tests. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Play or record test audio samples from various instruments covering diverse note ranges.
          \item Measure pitch detection accuracy by comparing transcribed notes to the actual pitches.
          \item Calculate the error rate and confirm it remains below 1\%.
      \end{enumerate}

    \item \textbf{Test for PR-PA2 Timing Accuracy} \\
      \newline
      \textbf{Test ID:} PR-PA2 \\
      \textbf{Type:} Functional, Performance \\
      \textbf{Initial State:} The app is set to capture audio input. \\
      \textbf{Input/Condition:} Test with audio samples that have precise note durations and rhythms. \\
      \textbf{Output/Result:} The app should capture note durations and rhythms with an accuracy tolerance within 100ms. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Play or record 
          \href{https://github.com/emilyperica/ScoreGen/tree/main/test/TestingDatasets/piano-samples/sample-songs}{audio samples} 
          with known timing and rhythm.
          \item Analyze the transcribed timing for each note and compare it to the original timing.
          \item Verify that timing discrepancies are within the 100ms tolerance limit.
      \end{enumerate}

    \item \textbf{Test for PR-RFT1 Reliability (Time Between Failures)} \\
      \newline
      \textbf{Test ID:} PR-RFT1 \\
      \textbf{Type:} Performance, Dynamic \\
      \textbf{Initial State:} The app is running under typical usage conditions. \\
      \textbf{Input/Condition:} Operate the app continuously for 24 hours. \\
      \textbf{Output/Result:} The app should function without crashes or failures for the entire 24-hour period in at least 
      95\% of cases. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Start the app under standard usage conditions and monitor it for 24 hours.
          \item Track and log any crashes or failures.
          \item Confirm that at least 95\% of the tests meet the requirement of continuous operation without interruption.
      \end{enumerate}

    \item \textbf{Test for PR-RFT2 Availability (Uptime)} \\
      \newline
      \textbf{Test ID:} PR-RFT2 \\
      \textbf{Type:} Performance, Static \\
      \textbf{Initial State:} The app is installed and operational over an extended period. \\
      \textbf{Input/Condition:} Monitor app uptime over several weeks. \\
      \textbf{Output/Result:} The app should demonstrate 99.5\% uptime, accounting for any brief planned downtime. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Log the app’s operational status over an extended period.
          \item Calculate the percentage of time the app is accessible and operational.
          \item Verify that uptime meets or exceeds 99.5\%.
      \end{enumerate}

    \item \textbf{Test for PR-RFT3 Crash Recovery} \\
      \newline
      \textbf{Test ID:} PR-RFT3 \\
      \textbf{Type:} Functional, Dynamic \\
      \textbf{Initial State:} The app is open and in active use with an ongoing transcription. \\
      \textbf{Input/Condition:} Simulate an unexpected crash. \\
      \textbf{Output/Result:} The app should automatically save the current session and allow users to recover their work 
      upon restarting, achieving 98\% recovery success. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Begin a transcription session and simulate a crash.
          \item Reopen the app and check if the session is restored, including any partially transcribed sheet music.
          \item Confirm that data recovery occurs in at least 98\% of test cases.
      \end{enumerate}
      \item \textbf{Test for PR-RFT4 Performance Under Load} \\
      \newline
      \textbf{Test ID:} PR-RFT4 \\
      \textbf{Type:} Performance, Dynamic \\
      \textbf{Initial State:} The app is running and ready to process complex or large inputs. \\
      \textbf{Input/Condition:} Test the app’s performance under maximum load conditions (e.g., 
      \href{https://github.com/emilyperica/ScoreGen/tree/main/test/TestingDatasets/piano-samples/sample-chords}{complex polyphonic inputs, large files}). \\
      \textbf{Output/Result:} The app should maintain stable performance with no more than a 30\% decrease in processing speed. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Load the app with large or complex audio files that simulate high activity conditions.
          \item Measure processing speed and performance metrics during this test.
          \item Verify that performance remains stable with no significant slowdowns or crashes and that any speed decrease 
          is within 30\%.
      \end{enumerate}

    \item \textbf{Test for PR-RFT5 Handling Signal Interruptions} \\
      \newline
      \textbf{Test ID:} PR-RFT5 \\
      \textbf{Type:} Functional, Dynamic \\
      \textbf{Initial State:} The app is receiving audio input from an external instrument or microphone. \\
      \textbf{Input/Condition:} Temporarily disconnect and then reconnect the audio input device. \\
      \textbf{Output/Result:} The app should retain buffered audio data for up to 2 minutes during interruptions and resume 
      transcription seamlessly. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Begin an audio input session and then simulate a signal interruption by disconnecting the audio source.
          \item Wait up to 2 minutes, then reconnect the audio source.
          \item Verify that buffered audio data is retained, and transcription resumes without any data loss in at least 95\% 
          of test cases.
      \end{enumerate}

    \item \textbf{Test for PR-RFT6 Graceful Degradation} \\
      \newline
      \textbf{Test ID:} PR-RFT6 \\
      \textbf{Type:} Performance, Dynamic \\
      \textbf{Initial State:} The app is running and under increasing system load. \\
      \textbf{Input/Condition:} Apply stress by loading multiple instruments or large files until performance begins to degrade. \\
      \textbf{Output/Result:} The app should notify the user of delays within 5 seconds of detection and avoid crashing in 99\% 
      of test cases. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Gradually increase system load by adding complex inputs or running multiple processes.
          \item Observe if the app notifies the user within 5 seconds when performance slows.
          \item Confirm the app remains operational without crashing in at least 99\% of test cases.
      \end{enumerate}

    \item \textbf{Test for PR-RFT7 Automatic Recovery from Software Glitches} \\
      \newline
      \textbf{Test ID:} PR-RFT7 \\
      \textbf{Type:} Functional, Dynamic \\
      \textbf{Initial State:} The app is running and processes files with varied input types. \\
      \textbf{Input/Condition:} Introduce minor software errors (e.g., 
      \href{https://github.com/emilyperica/ScoreGen/tree/main/test/TestingDatasets/sample-formats}{unexpected input format}). \\
      \textbf{Output/Result:} The app should log the error, notify the user, skip the problematic section, and continue 
      without crashing in 90\% of cases. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Feed the app corrupted or incompatible files.
          \item Verify that the app logs the error, notifies the user, and skips the problematic section.
          \item Confirm that the app continues functioning without crashing in at least 90\% of test cases.
      \end{enumerate}

    \item \textbf{Test for PR-C1 Audio Input Capacity} \\
      \newline
      \textbf{Test ID:} PR-C1 \\
      \textbf{Type:} Performance, Dynamic \\
      \textbf{Initial State:} The app is ready to record audio input. \\
      \textbf{Input/Condition:} Record audio continuously for up to 90 minutes. \\
      \textbf{Output/Result:} The app should process and transcribe the entire duration with no more than a 5\% slowdown 
      in speed or accuracy. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Start recording and let the app run continuously for 90 minutes.
          \item Monitor transcription speed and accuracy.
          \item Verify that performance remains consistent and within the 5\% slowdown limit.
      \end{enumerate}

    \item \textbf{Test for PR-C2 Simultaneous User Sessions} \\
      \newline
      \textbf{Test ID:} PR-C2 \\
      \textbf{Type:} Performance, Dynamic \\
      \textbf{Initial State:} The app supports simultaneous user sessions. \\
      \textbf{Input/Condition:} Run 10 simultaneous active user sessions. \\
      \textbf{Output/Result:} The app should maintain stable performance, response times, and transcription accuracy in 
      95\% of cases. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Initiate 10 user sessions, each running different tasks.
          \item Monitor performance metrics for each session, including response time and accuracy.
          \item Confirm that performance remains stable with no significant slowdown in 95\% of test cases.
      \end{enumerate}

    \item \textbf{Test for PR-SE2 Feature Extensibility} \\
      \newline
      \textbf{Test ID:} PR-SE2 \\
      \textbf{Type:} Structural, Static \\
      \textbf{Initial State:} The app’s codebase is available for review. \\
      \textbf{Input/Condition:} Evaluate the app’s modular architecture for future extensibility. \\
      \textbf{Output/Result:} The app’s design should allow feature additions without major refactoring. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Conduct a code review with development and architecture teams.
          \item Assess the modularity of the codebase and document potential areas for future feature integration.
          \item Confirm that new modules could be added with minimal impact on core functionalities.
      \end{enumerate}

    \item \textbf{Test for PR-SE3 Processing Power for Complex Music Compositions} \\
      \newline
      \textbf{Test ID:} PR-SE3 \\
      \textbf{Type:} Performance, Scalability \\
      \textbf{Initial State:} The app is prepared to process complex musical compositions. \\
      \textbf{Input/Condition:} Process 
      \href{https://github.com/emilyperica/ScoreGen/tree/main/test/TestingDatasets/piano-samples/sample-chords}{increasingly complex compositions} with multiple instrument tracks. \\
      \textbf{Output/Result:} The app should maintain stability and performance as complexity increases. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Gradually add instrument tracks and polyphonic elements to a composition.
          \item Monitor processing speed and stability.
          \item Confirm that the app handles increased complexity without significant performance issues.
      \end{enumerate}

    \item \textbf{Test for PR-L1 Expected Lifetime} \\
      \newline
      \textbf{Test ID:} PR-L1 \\
      \textbf{Type:} Structural, Static \\
      \textbf{Initial State:} The app’s development roadmap is complete. \\
      \textbf{Input/Condition:} Review the roadmap for planned updates and feature expansions over the next five years. \\
      \textbf{Output/Result:} The roadmap should ensure that the app remains functional and relevant for at least five 
      years with minor maintenance. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Review the app’s development roadmap with the team.
          \item Verify that updates, feature expansions, and maintenance plans are documented.
          \item Confirm that no major rewrites or overhauls are anticipated to keep the app functional and relevant.
      \end{enumerate}
\end{enumerate}
\subsubsection{Operational and Environmental Requirements Testing}
\begin{enumerate}
    \item \textbf{Test for OE-EP1 Operating Environment} \\
      \newline
      \textbf{Test ID:} OE-EP1 \\
      \textbf{Type:} Functional, Dynamic, Environmental \\
      \textbf{Initial State:} The app is running on a personal computer or laptop in a controlled indoor setting. \\
      \textbf{Input/Condition:} The app is operated in various typical indoor environments. \\
      \textbf{Output/Result:} The app should perform consistently without requiring any adjustments based on the environment. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Set up the app in indoor environments such as a home studio, classroom, and office.
          \item Record observations of the app's functionality in each environment, noting any issues related to lighting 
          or ambient noise.
          \item Confirm that the app functions as expected across all tested environments.
      \end{enumerate}

    \item \textbf{Test for OE-EP2 Noise and Audio Input Quality} \\
      \newline
      \textbf{Test ID:} OE-EP2 \\
      \textbf{Type:} Functional, Dynamic, Environmental \\
      \textbf{Initial State:} The app is configured to receive audio input in an environment with moderate 
      \href{https://github.com/emilyperica/ScoreGen/tree/main/test/TestingDatasets/noise-samples}{background noise}\citep*{noise}. \\
      \textbf{Input/Condition:} The app’s transcription accuracy is tested in environments with varying ambient noise levels. \\
      \textbf{Output/Result:} The app’s transcription accuracy should not degrade by more than 5\%. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Play a controlled background noise source in an environment and start the audio capture process.
          \item Record the transcription accuracy in the presence of moderate ambient noise.
          \item Calculate the error rate, confirming it remains within the acceptable threshold of no more than a 5\% drop in accuracy.
      \end{enumerate}

    \item \textbf{Test for OE-EP3 Workspace Flexibility} \\
      \newline
      \textbf{Test ID:} OE-EP3 \\
      \textbf{Type:} Usability, Dynamic, Environmental \\
      \textbf{Initial State:} The app is installed on devices with various screen sizes and resolutions, from 13-inch to 
      27-inch displays. \\
      \textbf{Input/Condition:} The app is run on screens of varying sizes to check layout flexibility. \\
      \textbf{Output/Result:} The app’s interface should be adaptable to each screen size and resolution, maintaining 
      full functionality. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Open the app on devices with screen sizes ranging from 13 to 27 inches.
          \item Check the layout, navigation, and accessibility of interactive elements on each screen size.
          \item Verify that all interactive elements remain visible and functional across screen sizes.
      \end{enumerate}

    \item \textbf{Test for OE-EP4 Portable Setup Compatibility} \\
      \newline
      \textbf{Test ID:} OE-EP4 \\
      \textbf{Type:} Usability, Dynamic, Environmental \\
      \textbf{Initial State:} The app is installed on a laptop in a temporary workspace, such as a cafe or live performance venue. \\
      \textbf{Input/Condition:} The app is used in transient environments to evaluate setup and operation without specialized 
      hardware. \\
      \textbf{Output/Result:} The app should function smoothly on standard laptop hardware, performing effectively in transient 
      conditions. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Set up the app on a laptop with standard specifications in a cafe or similar temporary workspace.
          \item Test the app’s functionality, including quick setup and breakdown, ensuring smooth operation.
          \item Document any usability issues encountered during setup and breakdown in the transient environment.
      \end{enumerate}

    \item \textbf{Test for OE-WE1 General Hardware} \\
      \newline
      \textbf{Test ID:} OE-WE1 \\
      \textbf{Type:} Usability, Dynamic, Environmental \\
      \textbf{Initial State:} The app is installed on devices with varying screen interfaces and sizes. \\
      \textbf{Input/Condition:} The app is run on screens from 13 to 27 inches with different resolutions. \\
      \textbf{Output/Result:} The app should be fully functional, maintaining usability across all screen sizes and resolutions. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Launch the app on devices with screen sizes ranging from 13 to 27 inches.
          \item Confirm that all UI elements are accessible, functional, and scale correctly on each screen size.
          \item Record any deviations in layout or usability and confirm functionality across all specified screen configurations.
      \end{enumerate}

    \item \textbf{Test for OE-IA1 Audio Input Devices} \\
      \newline
      \textbf{Test ID:} OE-IA1 \\
      \textbf{Type:} Functional, Dynamic \\
      \textbf{Initial State:} The app is installed and configured to receive audio input. \\
      \textbf{Input/Condition:} Connect standard audio input devices (USB microphone, instrument pickup, built-in microphone). \\
      \textbf{Output/Result:} The app should support audio input from USB Audio Class 1.0 and higher devices. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Connect various standard audio input devices, including USB microphones, instrument pickups, and built-in microphones.
          \item Record audio using each device and monitor the app’s performance.
          \item Verify that audio is captured successfully from all tested devices without connectivity issues.
      \end{enumerate}

    \item \textbf{Test for OE-IA2 Audio File Import and Export} \\
      \newline
      \textbf{Test ID:} OE-IA2 \\
      \textbf{Type:} Functional, Dynamic \\
      \textbf{Initial State:} The app is running and ready to import and export audio files. \\
      \textbf{Input/Condition:} Use sample audio files in WAV (PCM) and MP3 (MPEG-1 Layer III) formats for import and export. \\
      \textbf{Output/Result:} The app should successfully import and export audio files in WAV and MP3 formats. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Import WAV and MP3 files into the app and verify playback or analysis accuracy.
          \item Export audio files in WAV and MP3 formats, ensuring they are playable in standard audio players.
          \item Confirm that imported and exported files retain their quality and format specifications.
      \end{enumerate}

    \item \textbf{Test for OE-IA3 Music Notation Software Integration} \\
      \newline
      \textbf{Test ID:} OE-IA3 \\
      \textbf{Type:} Functional, Dynamic \\
      \textbf{Initial State:} The app has completed audio processing and is ready to export sheet music. \\
      \textbf{Input/Condition:} Generate sheet music and export in MusicXML format. \\
      \textbf{Output/Result:} The exported files should be compatible with popular music notation software. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Convert an \href{https://github.com/emilyperica/ScoreGen/tree/main/test/TestingDatasets/piano-samples/sample-songs}{audio sample} to sheet music within the app.
          \item Export the generated sheet music as a MusicXML file.
          \item Test the exported files in another music notation software (TBD) to ensure compatibility and accuracy.
      \end{enumerate}

    \item \textbf{Test for OE-P1 Distribution} \\
      \newline
      \textbf{Test ID:} OE-P1 \\
      \textbf{Type:} Functional, Static \\
      \textbf{Initial State:} The app is packaged and ready for distribution. \\
      \textbf{Input/Condition:} Package the app as a downloadable installer. \\
      \textbf{Output/Result:} The installer should be downloadable from a website or repository without additional dependencies. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Package the app into an executable installer.
          \item Upload the installer to a hosting site (TBD) and download it on test devices.
          \item Verify that installation completes successfully with no dependencies required.
      \end{enumerate}

    \item \textbf{Test for OE-P2 Installation Process} \\
      \newline
      \textbf{Test ID:} OE-P2 \\
      \textbf{Type:} Usability, Functional \\
      \textbf{Initial State:} The app installer is ready for use. \\
      \textbf{Input/Condition:} Begin installation with minimal technical guidance. \\
      \textbf{Output/Result:} The installation should be simple and guide users effectively. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Run the installer and proceed through the installation process.
          \item Confirm that instructions are clear and that users can install the app with minimal input.
          \item Verify that the app installs correctly and launches without issues.
      \end{enumerate}

    \item \textbf{Test for OE-P3 Size and Compatibility} \\
      \newline
      \textbf{Test ID:} OE-P3 \\
      \textbf{Type:} Functional, Static \\
      \textbf{Initial State:} The app installation file is prepared. \\
      \textbf{Input/Condition:} Check the installation file size and install it on various systems. \\
      \textbf{Output/Result:} The installation file should be 500MB or less, and the app should be compatible with different 
      system setups. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Confirm the installation file size does not exceed 500MB.
          \item Install the app on multiple devices with different operating systems and hardware specifications.
          \item Ensure the app operates smoothly across all tested systems.
      \end{enumerate}

    \item \textbf{Test for OE-P4 Post-Installation Configuration} \\
      \newline
      \textbf{Test ID:} OE-P4 \\
      \textbf{Type:} Usability, Functional \\
      \textbf{Initial State:} The app is installed and launched for the first time. \\
      \textbf{Input/Condition:} Access the settings configuration panel. \\
      \textbf{Output/Result:} Users should be able to modify settings from the configuration panel without editing files manually. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Launch the app and navigate to the settings configuration panel.
          \item Verify that users can adjust the input source and output format through the panel.
          \item Confirm that all settings save correctly and can be accessed again upon reopening.
      \end{enumerate}

    \item \textbf{Test for OE-R1 Initial Release} \\
      \newline
      \textbf{Test ID:} OE-R1 \\
      \textbf{Type:} Functional, Static \\
      \textbf{Initial State:} The app has completed development and quality assurance. \\
      \textbf{Input/Condition:} Conduct a final testing phase to ensure all core functionalities are operational. \\
      \textbf{Output/Result:} Core features should be functional with no major bugs, and ready for initial release. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Perform a full quality assurance test on the app, focusing on core functionality such as audio-to-sheet music 
          conversion in a production environment.
          \item Document any bugs or issues and resolve them before release.
          \item Verify that the app is stable and ready for general use by conducting a final user acceptance test.
      \end{enumerate}

\end{enumerate}
\subsubsection{Maintainability and Support Requirements Tests}
\begin{enumerate}
    \item \textbf{Test for MS-M1 Version Releases} \\
      \newline
      \textbf{Test ID:} MS-M1 \\
      \textbf{Type:} Functional, Dynamic \\
      \textbf{Initial State:} A new version of the app has just been released. \\
      \textbf{Input/Condition:} The user opens the app with an internet connection after a new release. \\
      \textbf{Output/Result:} The user should receive a notification to install the new version. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Release a test version update while the app is active on a connected device.
          \item Open the app on the test device and verify that the user receives a prompt to install the new version.
          \item Document any delays or failures in notification.
      \end{enumerate}

    \item \textbf{Test for MS-M2 System Crash} \\
      \newline
      \textbf{Test ID:} MS-M2 \\
      \textbf{Type:} Functional, Dynamic \\
      \textbf{Initial State:} The application is running and unexpectedly shuts down. \\
      \textbf{Input/Condition:} Force a non-user-prompted shutdown of the application. \\
      \textbf{Output/Result:} The app should reopen within 1 minute and recover data from the previous session. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Simulate an unexpected shutdown (e.g., force-close the app).
          \item Verify that the app automatically reboots within 1 minute.
          \item Confirm that data from the last session is preserved and accessible upon reopening.
      \end{enumerate}

    \item \textbf{Test for MS-S1 Software Bugs} \\
      \newline
      \textbf{Test ID:} MS-S1 \\
      \textbf{Type:} Functional, Dynamic \\
      \textbf{Initial State:} The user encounters a bug and accesses the reporting feature. \\
      \textbf{Input/Condition:} Submit a bug report via the app’s GUI. \\
      \textbf{Output/Result:} The development team should be notified within 1 hour of report submission. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Use the app’s bug reporting feature to submit a test report.
          \item Verify that the report is logged in the development team’s system within 1 hour.
          \item Document any delays or issues in the notification process.
      \end{enumerate}

    \item \textbf{Test for MS-S2 Operating System} \\
      \newline
      \textbf{Test ID:} MS-S2 \\
      \textbf{Type:} Functional, Static \\
      \textbf{Initial State:} The app is ready to be installed on various Windows versions. \\
      \textbf{Input/Condition:} Install and run the app on devices with Windows 10 and 11. \\
      \textbf{Output/Result:} The app should perform all expected functionalities on both Windows 10 and Windows 11. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Install the app on devices with Windows 10 and Windows 11.
          \item Confirm that all core functionalities (e.g., audio capture, transcription) work as expected on each operating system.
          \item Document any compatibility issues or functional limitations on each OS version.
      \end{enumerate}

    \item \textbf{Test for MS-A1 Internet Connection} \\
      \newline
      \textbf{Test ID:} MS-A1 \\
      \textbf{Type:} Functional, Dynamic \\
      \textbf{Initial State:} The app is running on a device with intermittent internet connectivity. \\
      \textbf{Input/Condition:} Test the app’s functionality both online and offline. \\
      \textbf{Output/Result:} The app should provide a consistent user experience regardless of internet connection status. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Launch the app and test core functionalities (e.g., audio processing, and navigation) with a stable internet connection.
          \item Disconnect the internet and continue using the app, verifying that the user experience remains consistent.
          \item Document any discrepancies in functionality when offline.
      \end{enumerate}

    \item \textbf{Test for MS-A2 GUI Navigation} \\
      \newline
      \textbf{Test ID:} MS-A2 \\
      \textbf{Type:} Usability, Functional \\
      \textbf{Initial State:} The app’s GUI is displayed with a connected keyboard and without a mouse. \\
      \textbf{Input/Condition:} Navigate the app’s interface solely using keyboard shortcuts. \\
      \textbf{Output/Result:} The app should remain fully functional and navigable without a mouse. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Disconnect any mouse from the test device and launch the app.
          \item Use keyboard shortcuts to navigate through each major feature and menu within the app.
          \item Confirm that all functionalities are accessible and that navigation is smooth using only the keyboard.
      \end{enumerate}
\end{enumerate}
\subsubsection{Security Requirement Tests}
\begin{enumerate}
    \item \textbf{Test for S-A1 User Authentication} \\
      \newline
      \textbf{Test ID:} S-A1 \\
      \textbf{Type:} Functional, Static \\
      \textbf{Initial State:} The application is closed. \\
      \textbf{Input/Condition:} Open the application as a standard user without any authentication. \\
      \textbf{Output/Result:} The application should allow access without requiring login credentials or password input. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Launch the application and verify that the user is granted access without needing to enter any login details.
          \item Document whether any authentication prompt appears unexpectedly.
          \item Confirm that the user can freely access all functionalities without an authentication barrier.
      \end{enumerate}

    \item \textbf{Test for S-P1 Data Storage} \\
      \newline
      \textbf{Test ID:} S-P1 \\
      \textbf{Type:} Functional, Static \\
      \textbf{Initial State:} The application is running, and the user has chosen a specific location for file storage. \\
      \textbf{Input/Condition:} Save an output file in the user-selected directory. \\
      \textbf{Output/Result:} The output file should be saved in the specified directory with full read and write permissions 
      for the user. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Configure the application to store output files in a user-defined location.
          \item Generate and save an output file, then verify that the file appears in the specified directory.
          \item Check that the user has read and write permissions for the saved file.
      \end{enumerate}

    \item \textbf{Test for S-P2 PII} \\
      \newline
      \textbf{Test ID:} S-P2 \\
      \textbf{Type:} Functional, Static \\
      \textbf{Initial State:} The application is running. \\
      \textbf{Input/Condition:} Use the application, observing all interactions for data input requests. \\
      \textbf{Output/Result:} The application should not request any Personal Identifiable Information (PII) from the user. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Launch the application and monitor for any prompts requesting personal data.
          \item Interact with all features of the app, verifying that no PII requests (e.g., name, address, or contact 
          information) are made.
          \item Document any instance where PII might be requested, and confirm the app remains free of such prompts.
      \end{enumerate}

    \item \textbf{Test for S-P3 Input Data} \\
      \newline
      \textbf{Test ID:} S-P3 \\
      \textbf{Type:} Functional, Dynamic \\
      \textbf{Initial State:} The application has processed an audio input file. \\
      \textbf{Input/Condition:} Complete an audio processing session. \\
      \textbf{Output/Result:} The application should clear all caches and temporary files associated with the processed 
      audio upon completion. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Process an \href{https://github.com/emilyperica/ScoreGen/tree/main/test/TestingDatasets/piano-samples}{audio file} 
          within the application.
          \item After processing, check the system’s temporary file storage to ensure no audio data or temporary files remain.
          \item Confirm that all caches are cleared, and no residual audio data is left on the system.
      \end{enumerate}
\end{enumerate}
\subsubsection{Cultural and Compliance Requirements Tests}
\begin{enumerate}
    \item \textbf{Test for CR-CR1 Musical Convention} \\
      \newline
      \textbf{Test ID:} CR-CR1 \\
      \textbf{Type:} Functional, Static \\
      \textbf{Initial State:} The application is open, ready to display and create sheet music. \\
      \textbf{Input/Condition:} Use the app to create and display sheet music, verifying adherence to Western music notation 
      \citep*{music-notation}. \\
      \textbf{Output/Result:} The generated and displayed sheet music should accurately follow Western music notation standards. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Generate a sample sheet music file using the app, incorporating various standard Western notation elements 
          (e.g., treble and bass clefs, notes, rests, time signatures).
          \item Compare the sheet music against standard Western notation guidelines, ensuring proper symbol usage and layout.
          \item Confirm that all notation adheres to Western music conventions without deviations or omissions.
      \end{enumerate}

    \item \textbf{Test for CR-CR2 Expected Music Theory Level of Users} \\
      \newline
      \textbf{Test ID:} CR-CR2 \\
      \textbf{Type:} Usability, Dynamic \\
      \textbf{Initial State:} The application is open and available to users with varying levels of music theory knowledge. \\
      \textbf{Input/Condition:} Users with basic to intermediate knowledge of music theory interact with the app. \\
      \textbf{Output/Result:} The app should be intuitive and accessible, allowing users to navigate and use core features comfortably. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Recruit a sample group of users with basic to intermediate music theory knowledge.
          \item Have users complete a task (e.g., creating a simple sheet of music) and observe their interaction with the app.
          \item Provide tutorials or guides as needed and gather feedback on their clarity and usefulness.
          \item Verify that users can successfully navigate and use the application without advanced music theory knowledge, 
          meeting usability expectations.
      \end{enumerate}

    \item \textbf{Test for CR-LR1 Copyright Issues} \\
      \newline
      \textbf{Test ID:} CR-LR1 \\
      \textbf{Type:} Functional, Static \\
      \textbf{Initial State:} The application is open, ready for user interaction. \\
      \textbf{Input/Condition:} Observe the app’s interface for the presence of a copyright disclaimer. \\
      \textbf{Output/Result:} The app should display a clear disclaimer advising users on copyright compliance related to 
      audio input. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Open the application and navigate to any sections that mention user responsibilities or usage terms.
          \item Confirm that a disclaimer is present, informing users of copyright restrictions and advising compliance.
          \item Verify that instructions are clear, guiding users on ensuring their audio inputs do not violate copyright laws.
      \end{enumerate}

    \item \textbf{Test for CR-SCR1 Technological Standards} \\
      \newline
      \textbf{Test ID:} CR-SCR1 \\
      \textbf{Type:} Functional, Static \\
      \textbf{Initial State:} The application has a completed sheet music file ready for export. \\
      \textbf{Input/Condition:} Export sheet music in MusicXML format and verify the use of third-party libraries. \\
      \textbf{Output/Result:} The app should export error-free MusicXML files and have documentation of all third-party libraries. \\
      \textbf{How Test Will Be Performed:}
      \begin{enumerate}
          \item Generate a sheet music file within the app and export it in MusicXML format.
          \item Open the exported file in compatible notation software (TBD) to confirm it is error-free and follows the 
          MusicXML standard.
          \item Review the app’s documentation to verify that all third-party libraries are listed with their licensing 
          terms for compliance purposes.
      \end{enumerate}
\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

For ease of traceability, test cases have been named such that each ID is identical to the ID of 
the functional or nonfunctional requirement it is intended to verify and validate. 
  
\begin{longtable}{|p{3cm}|p{4cm}|p{4cm}|}
  \caption{Requirements Traceability Matrix} \\
  \hline
  \textbf{SRS Location} & \textbf{Requirement ID} & \textbf{Test IDs} \\
  \hline
  \endfirsthead

  \hline
  \textbf{SRS Location} & \textbf{Requirement ID} & \textbf{Test IDs} \\
  \hline
  \endhead

  \hline
  \endfoot

  \hline
  \endlastfoot

  \multirow{3}{3cm}{Section 9.1} & FR-AR1 & FR-AR1-3-1 \newline FR-AR1-3-2 \\
  \cline{2-3}
  & FR-AR2 & FR-AR2 \\
  \cline{2-3}
  & FR-AR3 & FR-AR1-3-1 \newline FR-AR1-3-2 \\
  \hline
  \multirow{4}{3cm}{Section 9.2} & FR-SP1 & FR-SP1 \\
  \cline{2-3}
  & FR-SP2 & FR-SP2 \\
  \cline{2-3}
  & FR-SP3 & FR-SP3 \\
  \cline{2-3}
  & FR-SP4 & FR-SP4 \\
  \cline{2-3}
  & FR-SP5 & FR-SP2 \\
  \hline
  \multirow{3}{3cm}{Section 9.3} & FR-SMG1 & FR-SMG1 \\
  \cline{2-3}
  & FR-SMG2 & FR-SMG2 \\
  \cline{2-3}
  & FR-SMG3 & FR-SMG3 \\
  \hline
  \multirow{3}{3cm}{Section 9.4} & FR-UI1 & FR-UI1 \\
  \cline{2-3}
  & FR-UI2 & FR-UI2 \\
  \cline{2-3}
  & FR-UI3 & FR-UI3 \\
  \hline
  \multirow{2}{3cm}{Section 9.5} & FR-SL1 & FR-SL1 \\
  \cline{2-3}
  & FR-SL2 & FR-SL2 \\
  \hline
  \multirow{3}{3cm}{Section 10.1} 
  & LF-A1 & LF-A1 \\
  \cline{2-3}
  & LF-A2 & LF-A2 \\
  \cline{2-3}
  & LF-A3 & LF-A3 \\
  \hline
  \multirow{1}{3cm}{Section 10.2} 
  & LF-S1 & LF-S1 \\
  \hline
  \multirow{1}{3cm}{Section 11.1} 
  & UH-EOU1 & UH-EOU1 \\
  \hline
  \multirow{1}{3cm}{Section 11.2} 
  & UH-PI1 & UH-PI1 \\
  \hline
  \multirow{1}{3cm}{Section 11.3} 
  & UH-L1 & UH-L1 \\
  \hline
  \multirow{2}{3cm}{Section 11.4} 
  & UH-UP1 & UH-UP1 \\
  \cline{2-3}
  & UH-UP2 & UH-UP2 \\
  \hline
  \multirow{1}{3cm}{Section 11.5} 
  & UH-A1 & UH-A1 \\
  \hline
  \multirow{2}{3cm}{Section 12.1} 
  & PR-SL1 & PR-SL1 \\
  \cline{2-3}
  & PR-SL2 & PR-SL2 \\
  \hline
  \multirow{2}{3cm}{Section 12.2} 
  & PR-SC1 & PR-SC1 \\
  \cline{2-3}
  & PR-SC2 & PR-SC2 \\
  \hline
  \multirow{2}{3cm}{Section 12.3} 
  & PR-PA1 & PR-PA1 \\
  \cline{2-3}
  & PR-PA2 & PR-PA2 \\
  \hline
  \multirow{7}{3cm}{Section 12.4} 
  & PR-RFT1 & PR-RFT1 \\
  \cline{2-3}
  & PR-RFT2 & PR-RFT2 \\
  \cline{2-3}
  & PR-RFT3 & PR-RFT3 \\
  \cline{2-3}
  & PR-RFT3 & PR-RFT3 \\
  \cline{2-3}
  & PR-RFT3 & PR-RFT3 \\
  \cline{2-3}
  & PR-RFT3 & PR-RFT3 \\
  \cline{2-3}
  & PR-RFT3 & PR-RFT3 \\
  \hline
  \multirow{2}{3cm}{Section 12.5} 
  & PR-C1 & PR-C1 \\
  \cline{2-3}
  & PR-C2 & PR-C2 \\
  \hline
  \multirow{3}{3cm}{Section 12.6} 
  & PR-SE1 & Omitted \\
  \cline{2-3}
  & PR-SE2 & PR-SE2 \\
  \cline{2-3}
  & PR-SE3 & PR-SE3 \\
  \hline
  \multirow{1}{3cm}{Section 12.7} 
  & PR-L1 & PR-L1 \\
  \hline
  \multirow{4}{3cm}{Section 13.1} 
  & OE-EP1 & OE-EP1 \\
  \cline{2-3}
  & OE-EP2 & OE-EP2 \\
  \cline{2-3}
  & OE-EP3 & OE-EP3 \\
  \cline{2-3}
  & OE-EP4 & OE-EP4 \\
  \hline
  \multirow{1}{3cm}{Section 13.2} 
  & OE-WE1 & OE-WE1 \\
  \hline
  \multirow{3}{3cm}{Section 13.3} 
  & OE-IA1 & OE-IA1 \\
  \cline{2-3}
  & OE-IA2 & OE-IA2 \\
  \cline{2-3}
  & OE-IA3 & OE-IA3 \\
  \hline
  \multirow{4}{3cm}{Section 13.4} 
  & OE-P1 & OE-P1 \\
  \cline{2-3}
  & OE-P2 & OE-P2 \\
  \cline{2-3}
  & OE-P3 & OE-P3 \\
  \cline{2-3}
  & OE-P4 & OE-P4 \\
  \hline
  \multirow{1}{3cm}{Section 13.5} 
  & OE-R1 & OE-R1 \\
  \hline
  \multirow{2}{3cm}{Section 14.1} 
  & MS-M1 & MS-M1 \\
  \cline{2-3}
  & MS-M2 & MS-M2 \\
  \hline
  \multirow{2}{3cm}{Section 14.2} 
  & MS-S1 & MS-S1 \\
  \cline{2-3}
  & MS-S2 & MS-S2 \\
  \hline
  \multirow{2}{3cm}{Section 14.3} 
  & MS-A1 & MS-A1 \\
  \cline{2-3}
  & MS-A2 & MS-A2 \\
  \hline
  \multirow{1}{3cm}{Section 15.1} 
  & S-A1 & S-A1 \\
  \hline
  \multirow{1}{3cm}{Section 15.2} 
  & N/A & N/A \\
  \hline
  \multirow{3}{3cm}{Section 15.3} 
  & S-P1 & S-P1 \\
  \cline{2-3}
  & S-P2 & S-P2 \\
  \cline{2-3}
  & S-P3 & S-P3 \\
  \hline
  \multirow{1}{3cm}{Section 15.4} 
  & N/A & N/A \\
  \hline
  \multirow{1}{3cm}{Section 15.5} 
  & N/A & N/A \\
  \hline
  \multirow{2}{3cm}{Section 16.1} 
  & CR-CR1 & CR-CR1 \\
  \cline{2-3}
  & CR-CR1 & CR-CR1 \\
  \hline
  \multirow{1}{3cm}{Section 17.1} 
  & CR-LR1 & CR-LR1 \\
  \hline
  \multirow{1}{3cm}{Section 17.2} 
  & CR-SCR1 & CR-SCR1 \\
  \hline
\end{longtable}

\newpage

\section{Unit Test Description}
\label{sec:unit_tests}
This section will remain blank for revision 0 of the VnV plan, as the design document has not been completed yet.

% \wss{This section should not be filled in until after the MIS (detailed design
%   document) has been completed.}

% \wss{Reference your MIS (detailed design document) and explain your overall
% philosophy for test case selection.}  

% \wss{To save space and time, it may be an option to provide less detail in this section.  
% For the unit tests you can potentially layout your testing strategy here.  That is, you 
% can explain how tests will be selected for each module.  For instance, your test building 
% approach could be test cases for each access program, including one test for normal behaviour 
% and as many tests as needed for edge cases.  Rather than create the details of the input 
% and output here, you could point to the unit testing code.  For this to work, you code 
% needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}
N/A.

% \wss{What modules are outside of the scope.  If there are modules that are
%   developed by someone else, then you would say here if you aren't planning on
%   verifying them.  There may also be modules that are part of your software, but
%   have a lower priority for verification than others.  If this is the case,
%   explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}
N/A.

% \wss{Most of the verification will be through automated unit testing.  If
%   appropriate specific modules can be verified by a non-testing based
%   technique.  That can also be documented in this section.}

% \wss{Include a blurb here to explain why the subsections below cover the module.
%   References to the MIS would be good.  You will want tests from a black box
%   perspective and from a white box perspective.  Explain to the reader how the
%   tests were selected.}


\subsection{Traceability Between Test Cases and Modules for Unit Tests}
N/A.

% \wss{Provide evidence that all of the modules have been considered.}

\newpage

\addcontentsline{toc}{section}{Appendices}
\section*{Appendices}

\appendix

\section{Symbolic Parameters}
For Revision 0 we do not have any implementation details of the symbolic parameters for the tests, however, this section will be filled out as we progress.

\section{Usability Survey Questions}
The following usability survey will be provided to user testers alongside 
this VnV plan and the \href{https://github.com/emilyperica/ScoreGen/blob/main/docs/SRS-Volere/SRS.pdf}{SRS} document.

\subsubsection*{Part 1: User Demographics}
\begin{enumerate}[leftmargin=*]
  \item How old are you?
  \begin{itemize}
    \item 0-11
    \item 12-16
    \item 17-24
    \item 24+
  \end{itemize}
  \item What is your level of expertise in reading sheet music? 
  \begin{itemize}
    \item No knowledge whatsoever
    \item Beginner
    \item Intermediate
    \item Advanced
    \item Expert
  \end{itemize}
  \item How frequently do you use music transcription or audio analysis software?
  \begin{itemize}
    \item Daily
    \item Weekly
    \item Monthly
    \item Rarely
    \item Never
  \end{itemize}
  \item What is your primary purpose for using ScoreGen?
  \begin{itemize}
    \item Personal music practice
    \item Music transcription
    \item Composing original songs
    \item Academic research
    \item Other (please specify): \hrulefill
  \end{itemize}
\end{enumerate}

\subsubsection*{Part 2: Ease of Use}
\begin{enumerate}[resume, leftmargin=*]
  \item How intuitive was the user interface for ScoreGen?
  \begin{itemize}
    \item Very intuitive
    \item Somewhat intuitive
    \item Neutral
    \item Somewhat unintuitive
    \item Very unintuitive
  \end{itemize}
  \item Did you find the user guide and any in-application instructions sufficient for using the software?
  \begin{itemize}
    \item More than sufficient
    \item Sufficient
    \item Neutral 
    \item Somewhat insufficient
    \item Very insufficient
  \end{itemize}
  \item How easy was it to generate sheet music from an audio file?
  \begin{itemize}
    \item Very easy
    \item Somewhat easy
    \item Neutral 
    \item Somewhat difficult
    \item Very difficult
  \end{itemize}
  \item How easy was it to generate sheet music from live input (i.e., through your microphone or audio port)?
  \begin{itemize}
    \item Very easy
    \item Somewhat easy
    \item Neutral 
    \item Somewhat difficult
    \item Very difficult
  \end{itemize}
\end{enumerate}

\subsubsection*{Part 3: Performance and Accuracy}
\begin{enumerate}[resume, leftmargin=*]
  \item How accurate was the transcribed sheet music to the provided audio input?
  \begin{itemize}
    \item Very accurate
    \item Mostly accurate
    \item Somewhat inaccurate
    \item Very inaccurate
    \item I can’t read sheet music
  \end{itemize}
  \item Did the VnV plan cover sufficient cases for addressing the accuracy of a variety of audio types (e.g., different instruments, polyphonic vs. monophonic sounds)?
  \begin{itemize}
    \item More than sufficient
    \item Sufficient
    \item Neutral 
    \item Somewhat insufficient
    \item Very insufficient
  \end{itemize}
  \item Were there any audio types or formats where the system performed noticeable poorly?
  \begin{itemize}
    \item No
    \item Yes (please specify): \hrulefill
  \end{itemize}
  \item How satisfied were you with the speed of the software (e.g., start-up time, length of time between audio input and score generation)?
  \begin{itemize}
    \item Very satisfied
    \item Satisfied
    \item Neutral
    \item Dissatisfied
    \item Very dissatisfied
  \end{itemize}
\end{enumerate}

\subsubsection*{Part 4: Overall Satisfaction}
\begin{enumerate}[resume, leftmargin=*]
  \item Ho satisfied are you with ScoreGen as a whole?
  \begin{itemize}
    \item Very satisfied
    \item Satisfied
    \item Neutral
    \item Dissatisfied
    \item Very dissatisfied
  \end{itemize}
  \item Would you recommend this software to others?
  \begin{itemize}
    \item Definitely
    \item Probably
    \item Not sure
    \item Probably not
    \item Definitely not
  \end{itemize}
  \item If comfortable, please provide your favourite part of this experience.
  
  \vspace{5pt}
  \hrulefill
  \item If comfortable, please provide your least favourite part of this experience.
  
  \vspace{5pt}
  \hrulefill
  \item Any additional comments or suggestions?
  
  \vspace{5pt}
  \hrulefill
\end{enumerate}


\section{Supplementary Testing Information}
\subsection{Ommitted Testing}

While the majority of the functional and nonfunctional requirements have associated test cases, 
some were omitted for various reasons. This section explains their omission from the document. 
\begin{enumerate}
  \item FR-SP4 Processing Limitations \\
  Appears under SRS section 9.2 Signal Processing. \\
  This functional requirement does not
  have a corresponding test case. The nature of this functional requirement demands more technical knowledge
  about implementation details that have not yet been decided. The exact definition of 'overly complex audio input' shall
  remain ambiguous at this stage to prevent the overextension (or underextension) of the development team's
  capabilities during the project's irregular time constraints.\\
  \item PR-SE1 User Base Growth\\
  Appears under SRS section 12.6\\
  This nonfunctional requirement will not be tested as the scope of having 1000 users simultaneously using the application doesn't make sense
  for our use case.\\
\end{enumerate}

\subsection{Testing Data and Files}
Requirement test cases may have input that overlap with other test cases, as such, the testing data and files have not specifically been
assigned to any one test case. Testing data is available for any test case that may require them. \\
Test files and datasets are available under the \href{https://github.com/emilyperica/ScoreGen/tree/9203b49bd6b54247517192c6b1992f5fa952478f/test}{test} folder
in the project's GitHub repository \citep{sample-formats, piano-refscales-on-C, piano-wholetone-scale-on-C, piano-c-major-scales, piano-chord-progression1, piano-c-major-asc-desc, 
piano-hot-cross-buns-solo}. \\
\newpage

\addcontentsline{toc}{section}{References}
\bibliographystyle{plainnat}
\bibliography{../../refs/References}

\newpage

\addcontentsline{toc}{section}{Reflections}
\section*{Reflections}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}[leftmargin=*]
  \item What went well while writing this deliverable?
  
  \textbf{Mark} This deliverable went very well. I feel that our group was able to work separately on 
  different sections of the document, yet when the changes were merged together, the results fit well 
  together. This suggests we operate together smoothly without need of excessive feedback.

  \textbf{Ian} The effort and work we put into our initial SRS paid off very well for this deliverable, 
  especially when it came to functional requirements and the structure that was used in the SRS. This 
  structure and breakdown naturally fit well into the breakdown required by the creation of test cases 
  for specific testing areas. Additionally, I think our team communicated very well while writing this 
  deliverable, both individually for parts that were dependent on multiple members and team-wide.

  \textbf{Emily} Communication within the team for this deliverable was the best it has been to date. 
  It had been lacking during our last deliverable, which led to a less cohesive document and team-wide
  project issues not being fully adressed. This was a good wake-up call for us, and as a result we set
  explicit roles for all aspects of this deliverable, removing the concept of 'group' issues to make 
  sure we don't get stuck in the cycle of assuming someone else is getting things done.

  \textbf{Jackson} I think the best part of this deliverable was how clear the requirements were. I feel as 
  though as a group we are getting used to what we need to deliver to achieve success in each milestone, and in 
  this document in particular I found that we were easily able to divide up the work and complete it efficiently. 
  Additionally, trying to get the main content done a day before the submission deadline helped us to have a buffer 
  of time to fix things up.
  
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?

  \textbf{Mark} There were no particular pain points in this deliverable, though there was significant 
  volume of writing out each section, and there had to be significant recall of previous documents to 
  reference them where necessary.

  \textbf{Ian} The largest pain point for me was creating explicit differences between many test cases 
  that share not only the same input, but relatively similar expected results/output. Many of the tests 
  could cover multiple functional aspects but in order to keep the approximate 1:1 ratio of requirements 
  to test cases, this required deeper thought into how the test cases can be effectively differentiated. 
  Ultimately this was beneficial as it further separates functionalities that often depend on one another 
  due to the nature of the project.

  \textbf{Emily} My biggest pain point was getting over the concept of writing a verification and 
  validation plan before writing any code. I find it difficult to conceptualize abstract tests that are 
  not implementation-specific, so this document was good practice in creating system-wide tests, as 
  opposed to the unit tests I'm more comfortable with writing.

  \textbf{Jackson} The biggest pain point in my opinion was the high coupling between the NFR and FR sections. 
  This caused me to have to wait on creating some sections for the non-functional requirements because the 
  performance requirements specifically were very similar to the functional requirements. Additionally, we 
  had to make sure there were no merge conflicts when adding the mapping chart between test IDs and requirements.

  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member. 

  \vspace{5pt}
  Proper verification and validation of this project will require the team to gain both technical 
  and procedural knowledge on performing effectives tests. Gaining proficiency in testing frameworks 
  such as Google Test will allow the team to design and execute a comprehensive test suite, as 
  frameworks will become the base of our unit and integration tests, as well as in our implementation 
  of a regression test suite. The team will need to become comfortable with YAML syntax, as well as 
  its uses in configuration management, to ensure we use GitHub Actions for automated testing to the 
  fullest of its ability. Usage of YAMLs in testing can greatly improve modularity and will make the 
  process of changing test data very easy, if this is ever necessary. Static testing knowledge is a 
  vital aspect of VnV, and all team members will aim to increase their static testing capabilities, 
  focusing on code walkthroughs, inspections, and manual code reviews. Creating a VnV plan is useless 
  without thorough documentation for the VnV report, and thus a vital skill to this aspect of the project 
  is being able to write thorough documentation and taking detailed notes at every step of the process.

  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice? 

  \vspace{5pt}
  \textbf{Proficiency in testing frameworks}

  APPROACH 1: Apply a framework such as Google Test to some example project, and practice creating unit, 
  integration, and system tests. \\
  APPROACH 2: Enroll in and complete an online course covering the usage of a specific test framework.
  
  \textbf{Mark} I would prefer to practice some basic implementation, but begin applying the skills on the 
  actual project ASAP and learn as I go. Overall for the initial learning I’d rather use approach 1.

  \textbf{Emily} I often prefer learning by doing, rather than reading from a textbook, so I will pursue 
  approach 1.

  \textbf{Ian} Due to the tight timing, approach 1 is more attractive. I will pursue this approach since 
  I’m already enrolled in enough courses for the semester.

  \textbf{Jackson} Likely approach 1, I’d rather be hands-on.

  \vspace{10pt}
  \textbf{YAML Syntax}

  APPROACH 1: Practice writing YAML syntax by creating GHA workflows. \\
  APPROACH 2: Review YAML files of public projects, such as those from previous years of the capstone course 
  or large open-source projects found on GitHub.
  
  \textbf{Mark} I would prefer approach 2 where needed. It is faster for learning in the short term and I can 
  get the practical experience I need when working on crucial workflows for the project.

  \textbf{Emily} Approach 1 is more hands-on and thus applies more to my learning style than approach 2, so 
  this will be my preferred way of gaining YAML knowledge.

  \textbf{Ian} I will pursue approach 2 as it can provide me with a gauge of what is expected for this course 
  in terms of GHA and YAML files, while also allowing me 
  to view them in a variety of context-specific applications.

  \textbf{Jackson} Approach 2 is more attractive to me, I do like reading reference material if I know it's correct.

  \vspace{10pt}
  \textbf{Static testing knowledge}

  APPROACH 1: Identify various tools used for static testing (ex. SonarQube, ESLint) and incorporate them in 
  existing or example projects to get comfortable with their usage. \\
  APPROACH 2: Research and go over existing literature outlining static testing methodologies and best practices.

  \textbf{Mark} I would prefer Approach 2 to get a strong overview of the best principles to follow before applying 
  the skills on our capstone project.

  \textbf{Emily} I will pursue approach 1, as this method is very similar to how we will use static testing within our 
  project and can help me to get comfortable with static testing tools in a relevant environment.

  \textbf{Ian} Approach 1 makes more sense to me for the purposes of this course and the rate we should be improving 
  our static testing knowledge. Practice will accelerate this process.

  \textbf{Jackson} Approach 1 is better in the case of our project, and I'd rather do that than go through all that 
  documentation about 'best practices'.

  \vspace{10pt}
  \textbf{Documentation}

  APPROACH 1: Write up a variety of reports in different styles using dummy test results. \\
  APPROACH 2: Review course notes from SFWRENG 3RA3 as well as all standards described in Dr. Smith’s lectures.

  \textbf{Mark} I will choose approach 2 as it is much quicker and it is an effective way to grasp a lot of information quickly.

  \textbf{Emily} I will pursue approach 2, as the ease with which I can access the necessary knowledge sources will make 
  it simple for me to learn proper documentation guidelines.

  \textbf{Ian} I’ll choose approach 2 in this case as it’s more specific to the expectations of the course.

  \textbf{Jackson} Since I already took 3RA3, I believe I'd rather review my course notes as a starting point.

\end{enumerate}

\end{document}